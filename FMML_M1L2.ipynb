{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNADTnguTb3Y3CB0peAizWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshpesa/FMML/blob/main/FMML_M1L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rrsliKKc4ww2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)\n",
        "dataset = datasets.fetch_california_housing()\n",
        "dataset.target = dataset.target.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "def NN3(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argsort(dist)[:3]]\n",
        "    final_label = np.bincount(label).argmax()\n",
        "    return final_label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel\n",
        "\n",
        "def nn(traindata, trainlabel, testdata):\n",
        "    predlabel = np.array([NN3(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ],
      "metadata": {
        "id": "oaihS_3G8b0Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ],
      "metadata": {
        "id": "uwLCwJAO44rp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ],
      "metadata": {
        "id": "yRRTI7m18gcA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW4cAiV36oiM",
        "outputId": "dc5b3022-fece-480d-c522-c45aa5204712"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)"
      ],
      "metadata": {
        "id": "XgjO9uTU7NDN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the data based using 1 nearest neighbour and finding its accuracy\n",
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm :\", trainAccuracy*100, \"%\")\n",
        "\n",
        "#training the data based on 3 nearest neighbours and finding its accuracy\n",
        "trainpred1=nn(traindata, trainlabel, traindata)\n",
        "trainAccuracy1 = Accuracy(trainlabel, trainpred1)\n",
        "print(\"Training accuracy using 3 nearest neighbour algorithm :\", trainAccuracy1*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUfrP59Q7Nnm",
        "outputId": "9e6d0591-bec4-4406-96e7-673a76eb25bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm : 100.0 %\n",
            "Training accuracy using 3 nearest neighbour algorithm : 61.35996119016818 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "\n",
        "print(\"Test accuracy of nearest neighbour :\", testAccuracy*100, \"%\")\n",
        "\n",
        "testpred1 = nn(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy1 = Accuracy(testlabel, testpred1)\n",
        "\n",
        "print(\"Test accuracy of 3 nearest neighbours :\", testAccuracy1*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc8Oc2mS7hSQ",
        "outputId": "e8aaec9c-1beb-4e0c-ff55-9d41bdb68252"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy of nearest neighbour : 34.91795366795367 %\n",
            "Test accuracy of 3 nearest neighbours : 36.05212355212355 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise:\n",
        "\n",
        "\n",
        "Q1)How does the accuracy of the 3 nearest neighbour classifier change with the number of splits?\n",
        "\n",
        "A1)The accuracy increases\n",
        "\n",
        "Q2)How is it affected by the split size?\n",
        "\n",
        "A2)The accuracy increases with a larger split size\n",
        "\n",
        "\n",
        "Q3)Compare the results with the 1 nearest neighbour classifier.\n",
        "\n",
        "A3)The impact is less profound on the 1NN while it is more pronounced on a 3NN with increase in split size and no of splits"
      ],
      "metadata": {
        "id": "fUn60C1gEkRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Questions\n",
        "\n",
        "Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "\n",
        "Yes, it gives more consistent results\n",
        "\n",
        "\n",
        "Does it give more accurate estimate of test accuracy?\n",
        "\n",
        "Yes, it does with more number of splits and using cross validation.\n",
        "\n",
        "\n",
        "What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "\n",
        "Yes, but at a higher computing cost\n",
        "\n",
        "\n",
        "Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n",
        "\n",
        "No, as the dataset size needs to be large enough to get different varieties of splits for training and testing the data to get more accuracy."
      ],
      "metadata": {
        "id": "qbGJHvTbF5Ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Exercise: Try to implement a 3 nearest neighbour classifier and compare the accuracy of the 1 nearest neighbour classifier and the 3 nearest neighbour classifier on the test dataset. You can use the KNeighborsClassifier class from the scikit-learn library to implement the K-Nearest Neighbors model. You can set the number of neighbors using the n_neighbors parameter. You can also use the accuracy_score function from the scikit-learn library to calculate the accuracy of the model.\n"
      ],
      "metadata": {
        "id": "m63Ua4047kQk"
      }
    }
  ]
}